{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as g\n",
    "from pyspark.sql.functions import col\n",
    "import functionss as f\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "config(\"spark.memory.offHeap.enabled\",\"true\").\\\n",
    "config(\"spark.memory.offHeap.size\",\"10g\").\\\n",
    "enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "- In this project I will gather the data from four sources. I will load this data into staging dataframes. I will clean the raw data, write it to parquet files and perform an ETL process using a Spark cluster. Then I will write the data into Fact & Dimension tables to form a star schema. The star schema can then be used by the relevant parties to perform data analytics, correlation and ad-hoc reporting in an effective and efficient manner.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included?\n",
    "* i94 Immigration Sample Data: Sample data of immigration records from the US National Tourism and Trade Office. This data source will serve as the Fact table in the schema. This data comes from https://travel.trade.gov/research/reports/i94/historical/2016.html.\n",
    "* World Temperature Data world_temperature. This dataset contains temperature data in various cities from the 1700â€™s to 2013. Although the data is only recorded until 2013, we can use this as an average/gauge of temperature in 2017. This data comes from https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data.\n",
    "* US City Demographic Data: Data about the demographics of US cities. This dataset includes information on the population of all US cities such as race, household size and gender. This data comes from https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/.\n",
    "* Airport Codes: This table contains the airport codes for the airports in corresponding cities. This data comes from https://datahub.io/core/airport-codes#data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# IMMIGRATION DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\"immigration_data_sample.csv\",nrows=100).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### IMMIGRATION DATA\n",
    "\n",
    "### Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "|Feature|Description|\n",
    "|--------|-----------|\n",
    "|cicid|Unique ID|\n",
    "|i94yr|year|\n",
    "|i94mon|month|\n",
    "|i94cit|3 digit code for immigrant country of birth|\n",
    "|i94res|3 digit code for immigrant country of residence|\n",
    "|i94port|Port of admission|\n",
    "|arrdate|Arrival Date in the USA|\n",
    "|i94mode|Mode of transportation (1 = Air; 2 = Sea; 3 = Land; 9 = Not reported)|\n",
    "|i94addr|USA State of arrival|\n",
    "|depdate|Departure Date from the USA|\n",
    "|i94bir|Age of Respondent in Years|\n",
    "|i94visa|Visa codes collapsed into three categories|\n",
    "|count|Field used for summary statistics|\n",
    "|dtadfile|Character Date Field - Date added to I-94 Files|\n",
    "|visapost|Department of State where where Visa was issued|\n",
    "|occup|Occupation that will be performed in U.S|\n",
    "|entdepa|Arrival Flag - admitted or paroled into the U.S.|\n",
    "|entdepd|Departure Flag - Departed, lost I-94 or is deceased|\n",
    "|entdepu|Update Flag - Either apprehended, overstayed, adjusted to perm residence|\n",
    "|matflag|Match flag - Match of arrival and departure records|\n",
    "|biryear|4 digit year of birth|\n",
    "|dtaddto|Character Date Field - Date to which admitted to U.S. (allowed to stay until)|\n",
    "|gender|Non-immigrant sex|\n",
    "|insnum|INS number|\n",
    "|airline|Airline used to arrive in U.S.|\n",
    "|admnum|Admission Number|\n",
    "|fltno|Flight number of Airline used to arrive in U.S.|\n",
    "|visatype|Class of admission legally admitting the non-immigrant to temporarily stay in U.S|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# US CITIES DEMOGRAPHICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\"us-cities-demographics.csv\",sep=\";\",nrows=100).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### US CITIES DEMOGRAPHICS\n",
    "\n",
    "### Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "|Feature|Description|\n",
    "|--------|-----------|\n",
    "|City|City Name|\n",
    "|State|US State of the City|\n",
    "|Median Age|The median population age|\n",
    "|Male Population|Male population total|\n",
    "|Female Population|Female population total|\n",
    "|Total Population|Total population|\n",
    "|Number of Veterans|Number of veterans living in the city|\n",
    "|Foreign-born|Number of residents who were not born in the city|\n",
    "|Average Household Size|Average size of houses in the city|\n",
    "|State Code|Code of the state|\n",
    "|Race|Race class|\n",
    "|Count|Number of individuals in each race|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# GlobalLandTemperaturesByCity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\"GlobalLandTemperaturesByCity.csv\",nrows=100).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### TEMPERATURE DATA\n",
    "\n",
    "### Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "|Feature|Description|\n",
    "|--------|-----------|\n",
    "|dt|Date|\n",
    "|AverageTemperature|Average temperature in celsius|\n",
    "|AverageTemperatureUncertainty|95% confidence interval around average temperature|\n",
    "|City|Name of city|\n",
    "|Country|Name of country|\n",
    "|Latitude|Latitude of city|\n",
    "|Longitude|Longitude of city|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# AIRPORT CODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\"airport-codes_csv.csv\",nrows=100).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### AIRPORT CODES\n",
    "### Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "|Feature|Description|\n",
    "|--------|-----------|\n",
    "|ident|Unique identifier|\n",
    "|type|Airport type|\n",
    "|name|Airport name|\n",
    "|elevation_ft|Airport altitude|\n",
    "|continent|Continent|\n",
    "|iso_country|ISO Code of the airport's country|\n",
    "|iso_region|ISO Code for the airport's region|\n",
    "|municipality|City/Municipality where the airport is located|\n",
    "|gps_code|Airport GPS Code|\n",
    "|iata_code|Airport IATA Code|\n",
    "|local_code|Airport local code|\n",
    "|coordinates|Airport coordinates|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# IMMIGRATION DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_data=spark.read.parquet(\"sas_data\")\n",
    "pd.read_csv(\"immigration_data_sample.csv\",nrows=100).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "f.explore_data(immigration_data)\n",
    "cleaned_immigration_data=f.clean_null_values(immigration_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# US CITIES DEMOGRAPHICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demographics_data=spark.read.option(\"delimiter\", \";\").csv(\"us-cities-demographics.csv\",header=True)\n",
    "pd.read_csv(\"us-cities-demographics.csv\",sep=\";\",nrows=100).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demographics_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "f.explore_data(demographics_data)\n",
    "cleaned_demographics_data=f.clean_null_values(demographics_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# GlobalLandTemperaturesByCity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temprature_data=spark.read.csv(\"GlobalLandTemperaturesByCity.csv\",header=True,inferSchema=True)\n",
    "pd.read_csv(\"GlobalLandTemperaturesByCity.csv\",nrows=100).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temprature_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "f.explore_data(temprature_data)\n",
    "cleaned_temprature_data=f.clean_null_values(temprature_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# AIRPORT CODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "airport_data=spark.read.csv(\"airport-codes_csv.csv\",header=True,inferSchema=True)\n",
    "pd.read_csv(\"airport-codes_csv.csv\",nrows=100).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "airport_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "f.explore_data(airport_data)\n",
    "cleaned_airport_data=f.clean_null_values(airport_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "<img title=\"\" alt=\"\" src=\"ER.png\">\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "> #### List the steps necessary to pipeline the data into the chosen data model\n",
    "> - ##### Load the data into staging dataframe\n",
    "> - ##### clean data from null values and duplicates\n",
    "> - ##### Create Dimension tables\n",
    "> - ##### Create Fact table\n",
    "> - ##### Write data into parquet files\n",
    "> - ##### Perform data quality checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### create immigration fact table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "f.process_visa(cleaned_immigration_data,\"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### create demographics dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "f.process_demographic(cleaned_demographics_data,\"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### create temprature dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "f.process_Temperature(cleaned_temprature_data,\"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### create airport dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "f.process_airport(cleaned_airport_data,\"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### create visa dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "f.process_visa(cleaned_immigration_data,\"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### create applicant_nationalty and admission_port dimension tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "applicant_nationalty=spark.read.csv(\"applicant_nationalty_table.csv\",header=True)\n",
    "admission_port=spark.read.csv(\"admission_port_table.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "admission_port.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "f.process_applicant_nationality(applicant_nationalty,\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "f.process_addmission_port(admission_port,\"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration=spark.read.parquet(\"data/fact_immigration\")\n",
    "visa = spark.read.parquet(\"data/dim_visa\")\n",
    "temperature = spark.read.parquet(\"data/dim_temprature\")\n",
    "demographic = spark.read.parquet(\"data/dim_demographic\")\n",
    "admission_port = spark.read.parquet(\"data/dim_addmission_port\")\n",
    "airport = spark.read.parquet(\"data/dim_airport\")\n",
    "applicant_nationality = spark.read.parquet(\"data/dim_applicant_nationality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration.show(2,vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "tables = {\n",
    "    \"immigration\": immigration,\n",
    "    \"airport\": airport,\n",
    "    \"demographic\": demographic,\n",
    "    \"temperature\": temperature,\n",
    "    \"applicant_nationality\": applicant_nationality,\n",
    "    \"admission_port\": admission_port,\n",
    "    \"visa\": visa}\n",
    "\n",
    "for table_name, table in tables.items():\n",
    "    f.perform_quality_check(table, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "please check data_dictionary.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Clearly state the rationale for the choice of tools and technologies for the project:\n",
    "- #### This project makes use of various Big Data processing technologies including:\n",
    "- #### Apache Spark, because of its ability to process massive amounts of data as well as the use of its unified analytics engine and convenient APIsPandas, due to its convenient dataframe manipulation functions to gain further insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Propose how often the data should be updated and why:\n",
    "- #### The immigration (i94) data set is updated monthly, hence all relevant data should be updated monthly as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### The data was increased by 100x:\n",
    "- #### If the data was increased by 100x I would use more sophisticated and appropriate frameworks to perform processing and storage functions, such as Amazon Redshift, Amazon EMR or Apache Cassandra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### The data populates a dashboard that must be updated on a daily basis by 7am every day:\n",
    "- #### If the data had to populate a dashboard daily, I would manage the ETL pipeline in a DAG from Apache Airflow. This would ensure that the pipeline runs in time, that data quality checks pass, and provide a convenient means of notification should the pipeline fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### The database needed to be accessed by 100+ people:\n",
    "- #### If the data needed to be accessed by many people simultaneously, I would move the analytics database to Amazon Redshift which can handle massive request volumes and is easily scalable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
